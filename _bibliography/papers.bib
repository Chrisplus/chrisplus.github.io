---
---

@inproceedings{turbo_sensys_22,
author = {Lu, Yan and Jiang, Shiqi and Cao, Ting and Shu, Yuanchao},
title = {Turbo: Opportunistic Enhancement for Edge Video Analytics},
year = {2022},
isbn = {9781450398862},
publisher = {Association for Computing Machinery},
address = {Boston, MA, USA},
url = {https://doi.org/10.1145/3560905.3568501},
doi = {10.1145/3560905.3568501},
abstract = {Edge computing is being widely used for video analytics. To alleviate the inherent tension between accuracy and cost, various video analytics pipelines have been proposed to optimize the usage of GPU on edge nodes. Nonetheless, we find that GPU compute resources provisioned for edge nodes are commonly under-utilized due to video content variations, subsampling and filtering at different places of a video analytics pipeline. As opposed to model and pipeline optimization, in this work, we study the problem of opportunistic data enhancement using the non-deterministic and fragmented idle GPU resources. In specific, we propose a task-specific discrimination and enhancement module,  and a model-aware adversarial training mechanism, providing a way to exploit idle resources to identify and transform pipeline-specific, low-quality images in an accurate and efficient manner. A multi-exit enhancement model structure and a resource-aware scheduler is further developed to make online enhancement decisions and fine-grained inference execution under latency and GPU resource constraints. Experiments across multiple video analytics pipelines and datasets reveal that our system boosts DNN object detection accuracy by $7.27-11.34\%$ by judiciously allocating $15.81-37.67\%$ idle resources on frames that tend to yield greater marginal benefits from enhancement. },
booktitle = {Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems},
series = {SenSys '22},
abbr={SenSys '22},
selected={true},
bibtex_show={true},
pdf={sensys22-Turbo.pdf},
code={https://aka.ms/turbo-project},
}

@inproceedings{codl_mobisys_22,
author = {Jia, Fucheng and Zhang, Deyu and Cao, Ting and Jiang, Shiqi and Liu, Yunxin and Ren, Ju and Zhang, Yaoxue},
title = {CoDL: Efficient CPU-GPU Co-Execution for Deep Learning Inference on Mobile Devices},
year = {2022},
isbn = {9781450391856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3498361.3538932},
doi = {10.1145/3498361.3538932},
abstract = {Concurrent inference execution on heterogeneous processors is critical to improve the performance of increasingly heavy deep learning (DL) models. However, available inference frameworks can only use one processor at a time, or hardly achieve speedup by concurrent execution compared to using one processor. This is due to the challenges to 1) reduce data sharing overhead, and 2) properly partition each operator between processors.By solving the challenges, we propose CoDL, a concurrent DL inference framework for the CPU and GPU on mobile devices. It can fully utilize the heterogeneous processors to accelerate each operator of a model. It integrates two novel techniques: 1) hybrid-type-friendly data sharing, which allows each processor to use its efficient data type for inference. To reduce data sharing overhead, we also propose hybrid-dimension partitioning and operator chain methods; 2) non-linearity- and concurrency-aware latency prediction, which can direct proper operator partitioning by building an extremely light-weight but accurate latency predictor for different processors.Based on the two techniques, we build the end-to-end CoDL inference framework, and evaluate it on different DL models. The results show up to 4.93\texttimes{} speedup and 62.3% energy saving compared with the state-of-the-art concurrent execution system.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services},
pages = {209–221},
numpages = {13},
keywords = {mobile devices, deep learning inference, CPU-GPU co-execution},
location = {Portland, Oregon},
series = {MobiSys '22},
abbr={MobiSys '22},
selected={true},
bibtex_show={true},
pdf={mobisys22-CoDL.pdf},
code={https://github.com/csu-eis/CoDL},
}


@inproceedings{remix_mobicom_21,
author = {Jiang, Shiqi and Lin, Zhiqi and Li, Yuanchun and Shu, Yuanchao and Liu, Yunxin},
title = {Flexible High-Resolution Object Detection on Edge Devices with Tunable Latency},
year = {2021},
isbn = {9781450383424},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447993.3483274},
doi = {10.1145/3447993.3483274},
abstract = {Object detection is a fundamental building block of video analytics applications. While Neural Networks (NNs)-based object detection models have shown excellent accuracy on benchmark datasets, they are not well positioned for high-resolution images inference on resource-constrained edge devices. Common approaches, including down-sampling inputs and scaling up neural networks, fall short of adapting to video content changes and various latency requirements. This paper presents Remix, a flexible framework for high-resolution object detection on edge devices. Remix takes as input a latency budget, and come up with an image partition and model execution plan which runs off-the-shelf neural networks on non-uniformly partitioned image blocks. As a result, it maximizes the overall detection accuracy by allocating various amount of compute power onto different areas of an image. We evaluate Remix on public dataset as well as real-world videos collected by ourselves. Experimental results show that Remix can either improve the detection accuracy by 18%-120% for a given latency budget, or achieve up to 8.1\texttimes{} inference speedup with accuracy on par with the state-of-the-art NNs.},
booktitle = {Proceedings of the 27th Annual International Conference on Mobile Computing and Networking},
pages = {559–572},
numpages = {14},
keywords = {deep neural networks, edge computing, object detection, video analytics, tunable latency},
location = {New Orleans, Louisiana},
series = {MobiCom '21},
abbr={MobiCom '21},
selected={true},
bibtex_show={true},
pdf={mobicom21-Remix.pdf},
slides={Remix_Mobicom_Present.pdf},
}

@inproceedings{profilfing_mobile_gpu_apsys_20,
author = {Jiang, Shiqi and Ran, Lihao and Cao, Ting and Xu, Yusen and Liu, Yunxin},
title = {Profiling and Optimizing Deep Learning Inference on Mobile GPUs},
year = {2020},
isbn = {9781450380690},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409963.3410493},
doi = {10.1145/3409963.3410493},
abstract = {Mobile GPU, as the ubiquitous computing hardware on almost every smartphone, is being exploited for the deep learning inference. In this paper, we present our measurements on the inference performance with mobile GPUs. Our observations suggest that mobile GPUs are underutilized. We study the inefficient issue in depth and find that one of root causes is the improper partition of compute workload. To solve this, we propose a heuristics-based workload partitioning approach, considering both performance and overheads on mobile devices. Evaluation results show that our approach reduces the inference latency by up to 32.8% on various devices and neural networks.},
booktitle = {Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems},
pages = {75–81},
numpages = {7},
keywords = {workload partition, mobile GPU, deep learning inference},
location = {Tsukuba, Japan},
series = {APSys '20},
abbr={APSys '20},
selected={true},
bibtex_show={true},
pdf={apsys20-profiling.pdf}
}

@article{memento_tosn,
author = {Jiang, Shiqi and Li, Zhenjiang and Zhou, Pengfei and Li, Mo},
title = {Memento: An Emotion-Driven Lifelogging System with Wearables},
year = {2019},
issue_date = {February 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {1},
issn = {1550-4859},
url = {https://doi.org/10.1145/3281630},
doi = {10.1145/3281630},
abstract = {Due to the increasing popularity of mobile devices, the usage of lifelogging has dramatically expanded. People collect their daily memorial moments and share with friends on the social network, which is an emerging lifestyle. We see great potential of lifelogging applications along with rapid recent growth of the wearables market, where more sensors are introduced to wearables, i.e., electroencephalogram (EEG) sensors, that can further sense the user’s mental activities, e.g., emotions. In this article, we present the design and implementation of Memento, an emotion-driven lifelogging system on wearables. Memento integrates EEG sensors with smart glasses. Since memorable moments usually coincides with the user’s emotional changes, Memento leverages the knowledge from the brain-computer-interface domain to analyze the EEG signals to infer emotions and automatically launch lifelogging based on that. Towards building Memento on Commercial off-the-shelf wearable devices, we study EEG signals in mobility cases and propose a multiple sensor fusion based approach to estimate signal quality. We present a customized two-phase emotion recognition architecture, considering both the affordability and efficiency of wearable-class devices. We also discuss the optimization framework to automatically choose and configure the suitable lifelogging method (video, audio, or image) by analyzing the environment and system context. Finally, our experimental evaluation shows that Memento is responsive, efficient, and user-friendly on wearables.},
journal = {ACM Transactions on Sensor Networks},
month = {jan},
articleno = {8},
numpages = {23},
keywords = {lifelogging, emotion recognition, Wearable, EEG},
abbr={ACM TOSN},
selected={true},
bibtex_show={true}
}

@ARTICLE{power_bus_rider_tits,
author={Liu, Zhidan and Jiang, Shiqi and Zhou, Pengfei and Li, Mo},  
journal={IEEE Transactions on Intelligent Transportation Systems},   
title={A Participatory Urban Traffic Monitoring System: The Power of Bus Riders},   
year={2017},  
volume={18},  
number={10},  
pages={2851-2864},  
doi={10.1109/TITS.2017.2650215},
abstract={This paper presents a participatory sensing-based urban traffic monitoring system. Different from existing works that heavily rely on intrusive sensing or full cooperation from probe vehicles, our system exploits the power of participatory sensing and crowdsources the traffic sensing tasks to bus riders' mobile phones. The bus riders are information source providers and, meanwhile, major consumers of the final traffic output. The system takes public buses as dummy probes to detect road traffic conditions, and collects the minimum set of cellular data together with some lightweight sensing hints from the bus riders' mobile phones. Based on the crowdsourced data from participants, the system recovers the bus travel information and further derives the instant traffic conditions of roads covered by bus routes. The real-world experiments with a prototype implementation demonstrate the feasibility of our system, which achieves accurate and fine-grained traffic estimation with modest sensing and computation overhead at the crowd.},
abbr={IEEE TITS},
selected={true},
bibtex_show={true}
}

@INPROCEEDINGS{urban_traffic_icdcs,  
author={Zhou, Pengfei and Jiang, Shiqi and Li, Mo},  
booktitle={2015 IEEE 35th International Conference on Distributed Computing Systems},   
title={Urban Traffic Monitoring with the Help of Bus Riders},   
year={2015},  
pages={21-30},  
abstract={Real-time urban traffic conditions are critical to wide populations in the city and serve the needs of many transportation dependent applications. This paper presents our experience of building a participatory urban traffic monitoring system that exploits the power of bus riders' mobile phones. The system takes lightweight sensor hints and collects minimum set of cellular data from the bus riders' mobile phones. Based on such a participatory sensing framework, the system turns buses into dummy probes, monitors their travel statuses, and derives the instant traffic map of the city. Unlike previous works that rely on intrusive detection or full cooperation from "probe vehicles", our approach resorts to the crowd-participation of ordinary bus riders, who are the information source providers and major consumers of the final traffic output. The experiment results demonstrate the feasibility of such an approach achieving fine-grained traffic estimation with modest sensing and computation overhead at the crowd.},  
doi={10.1109/ICDCS.2015.11},  
ISSN={1063-6927},  
month={June},
abbr={ICDCS '15},
selected={false},
}

@article{doi:10.1155/2015/135150,
author = {Shiqi Jiang and Pengfei Zhou and Mo Li},
title ={Detecting Phantom Data Usage on Smartphones with Analysis of Contextual Information},
journal = {International Journal of Distributed Sensor Networks},
volume = {11},
number = {11},
pages = {135150},
year = {2015},
doi = {10.1155/2015/135150},
abstract = { With the wide development of smartphones, mobile data usage has enjoyed rapid growth in recent years. Unfortunately many users are plagued with Phantom Data Usage (PDU), which refers to the unexpected mobile data usage that does not accord with user perception. We investigate about 400 real PDU issues and find the causes of PDU are not only the exceptions of applications, for example, software bugs or malware, but also the user's personalized misuse. Based on the observations that each user preserves specific data usage patterns under particular environmental context, we present PDS, a PDU detection system, which automatically detects whether the current data usage is consumed as expected. Results from our evaluation experiments show that 72\% of PDU cases detected by PDS are confirmed by users. },
abbr={IJDSN},
selected={false},
}

@inproceedings{10.1145/2639108.2641738,
author = {Zhou, Pengfei and Chan, Weiming and Jiang, Shiqi and Ou, Jiajue and Li, Mo and Shen, Guobin},
title = {Demo: Instant Phone Attitude Estimation and Its Applications},
year = {2014},
isbn = {9781450327831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639108.2641738},
doi = {10.1145/2639108.2641738},
abstract = {The phone attitude is an essential input to many smartphone applications. Based on in-depth understanding of the nature of the MEMS gyroscope and other IMU sensors, we propose A3 - an accurate and automatic attitude detector for commodity smartphones. In the demo, we show the performance of our attitude tracking algorithm and its usability in attitude-based mobile applications.},
booktitle = {Proceedings of the 20th Annual International Conference on Mobile Computing and Networking},
pages = {333–336},
numpages = {4},
keywords = {gyroscope, imu sensors, attitude-based applications, mobile phone attitude},
location = {Maui, Hawaii, USA},
series = {MobiCom '14},
abbr={MobiCom '14},
selected={false},
}